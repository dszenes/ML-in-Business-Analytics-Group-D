# 4 Analysis


```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

## 4.1 Research question 1

- Let's put ourselves in context. Our first research question is as follows:

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

RQ

</div>

<br><br><br>

...

<br>


## Unsupervised Learning

```{r}
library(ggcorrplot)
ggcorrplot(cor(final_df[1:19]))
```

```{r}
summary(prcomp(final_df[1:18], scale = TRUE))
library(FactoMineR)
library(factoextra)
pop.pca <- PCA(final_df[1:18], ncp = 21, graph = FALSE)

fviz_pca_var(pop.pca)
fviz_eig(pop.pca, addlabels = TRUE, ncp=21)
for (i in 1:6) {
  print(fviz_contrib(pop.pca, choice = "var", axes = i))
}
```

## Supervised Learning

### Binary Classes
```{r}
set.seed(1996) ## for replication purpose
#split the data set in training/test set
index.tr <- createDataPartition(y = final_df$share_label, p = 0.8, list = FALSE)
final_df.tr <- final_df[index.tr,]
final_df.te <- final_df[-index.tr,]
```

```{r}
library(caret)
final_df.knn <- knn3(data = final_df.tr, share_label ~., k = 5)
final_df.knn.pred <- predict(final_df.knn, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(final_df.knn.pred),
                reference = final_df.te$share_label)
                
                
                

#  Tuning of hyperparameters 


# Tuning of hyperparameters using grid search

grid <- expand.grid(k = c(1,10,50,100,150,200))
trctr <- trainControl(method = "cv", number = 10)
set.seed(1996)
Doc.knn <- train(share_label~., data = final_df.tr, method = "knn",
                 trControl= trctr,
                 tuneGrid = grid) 


# too much time to run
# try another method:

set.seed(1996)
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
Fit <- train(share_label~., data = final_df.tr, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 10)

Fit
# too much time to run too...
# how to solve this problem?

# Try to change manually the k and seeing the results:
# k = 10

final_df.knn2 <- knn3(data = final_df.tr, share_label ~., k = 10)
final_df.knn.pred2 <- predict(final_df.knn2, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(final_df.knn.pred2),
                reference = final_df.te$share_label)

# slightly better accuracy

# try with k = 100

final_df.knn3 <- knn3(data = final_df.tr, share_label ~., k = 100)
final_df.knn.pred3 <- predict(final_df.knn3, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(final_df.knn.pred3),
                reference = final_df.te$share_label)
                
# slightely better accuracy but the problem is that the improvement is almost useless. Accuracy is too low
                
```

```{r}
library(e1071)
final_df.svm <- svm(share_label ~ ., data=final_df.tr, kernel="linear")
final_df.svm.pred <- predict(final_df.svm, newdata = final_df.te)
confusionMatrix(data = as.factor(final_df.svm.pred),
                reference = final_df.te$share_label)

#Trying a radial basis kernel
final_df.svm_radial <- svm(share_label ~ ., data=final_df.tr, kernel="radial")
final_df.svm.pred_radial <- predict(final_df.svm_radial, newdata = final_df.te)
confusionMatrix(data = as.factor(final_df.svm.pred_radial),
                reference = final_df.te$share_label)

#Tuning the parameters
trctrl <- trainControl(method = "cv", number=10)
set.seed(143)
final_df.svm_linear <- train(share_label ~., data = final_df.tr, method = "svmLinear",trControl=trctrl)

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid
set.seed(143)
final_df.svm_Linear_Grid <- train(share_label ~., data = df.tr, method = "svmLinear",trControl=trctrl,tuneGrid = grid)

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial
set.seed(143)
final_df.svm_Radial_Grid <- train(share_label ~., data = df.tr, method = "svmRadial",trControl=trctrl,tuneGrid = grid_radial)
```

```{r}
library(naivebayes)
final_df.nb <- naive_bayes(share_label ~ ., data = final_df.tr, kernel = TRUE)
final_df.nb.pred <- predict(final_df.nb, newdata = final_df.te)
confusionMatrix(data = as.factor(final_df.nb.pred),
                reference = final_df.te$share_label)
```

```{r}
library(rpart)
library(rpart.plot)
final_df.tree <- rpart(share_label ~ ., data=final_df.tr)
rpart.plot(final_df.tree)
final_df.tree.pred <- predict(final_df.tree, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(final_df.tree.pred),
                reference = final_df.te$share_label)

#Trying if pruning is needed

plotcp(final_df.tree)

#No need to prune
```

```{r}
library(randomForest)
rf <- randomForest(share_label ~ ., data = final_df.tr)
rf.te.pred <- predict(rf, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(rf.te.pred),
                reference = final_df.te$share_label)

```

Since random forest seems to be the best model, we will do a cross-validation on it to verify the accuracy

```{r}

library('rfUtilities')
rf.cv <- rf.crossValidation(rf, final_df.tr[,-19], p=0.10, n=10, ntree=500)


```

### Multi-Classes : Searching for outstanding popularity
```{r}
set.seed(1996) ## for replication purpose
#split the data set in training/test set
c.index.tr <- createDataPartition(y = classes_df$share_classes, p = 0.8, list = FALSE)
classes_df.tr <- classes_df[c.index.tr,]
classes_df.te <- classes_df[-c.index.tr,]
```

```{r}
df.tr.unpop <- filter(classes_df.tr, share_classes =="Unpopular")
df.tr.pop <- filter(classes_df.tr, share_classes =="Popular")
df.tr.out <- filter(classes_df.tr, share_classes =="Outstanding")

n.out <- min(table(classes_df.tr$share_classes)) #1582

index.unpop <- sample(size=n.out, x=1:nrow(df.tr.unpop), replace=FALSE) ## sub-sample 840 instances from the "No"
index.pop <- sample(size=n.out, x=1:nrow(df.tr.pop), replace=FALSE) ## sub-sample 840 instances from the "No"

df.tr.subs <- data.frame(rbind(df.tr.out, df.tr.unpop[index.unpop,], df.tr.pop[index.pop,]))
table(df.tr.subs$share_classes)
```

```{r}
library(caret)
class_df.knn <- knn3(data = df.tr.subs, share_classes ~., k = 5)
class_df.knn.pred <- predict(class_df.knn, newdata = classes_df.te, type = "class")
confusionMatrix(data = as.factor(class_df.knn.pred),
                reference = classes_df.te$share_classes)
```



```{r}
# Variable importance random forest
library(randomForest)
rf <- randomForest(share_label ~ ., data = final_df.tr)

varImpPlot(rf)

# some minutes to run but it works

# Gini coefficient is an impurity measure. The interpretation of the plot is the following: on the top the most important variables that contribute the most to decrease the node impurity. self_reference.avg_sharess is the most important variable and "day_is_weekend" is the less important.

# variable importance CART
# without shuffle 
library(rpart)
library(rpart.plot)
final_df.tree <- rpart(share_label ~ ., data=final_df.tr)
rpart.plot(final_df.tree)
final_df.tree.pred <- predict(final_df.tree, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(final_df.tree.pred),
                reference = final_df.te$share_label)

# shuffle on self_reference_avg_sharess

set.seed(143)

final_df.te_shuffle <- final_df.te
final_df.te_shuffle$self_reference_avg_sharess <- sample(final_df.te_shuffle$self_reference_avg_sharess)

final_df.tree.pre_shuffle <- predict(final_df.tree, newdata = final_df.te_shuffle, type = "class")

confusionMatrix(data = as.factor(final_df.tree.pre_shuffle), reference = final_df.te$share_label)

# Looking at the output we see that the accuracy change from 0.613 to 0.566 ( the latter is in the shuffle procedure)

```



