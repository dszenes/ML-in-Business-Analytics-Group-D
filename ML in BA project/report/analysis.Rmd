# 4 Analysis


```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

## 4.1 Research question 1

- Let's put ourselves in context. Our first research question is as follows:

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

RQ

</div>

<br><br><br>

...

<br>


## Unsupervised Learning

```{r}
ggcorrplot(cor(final_df[1:18]))
```

```{r}
summary(prcomp(final_df[1:18], scale = TRUE))
pop.pca <- PCA(final_df[1:18], ncp = 21, graph = FALSE)

fviz_pca_var(pop.pca)
fviz_eig(pop.pca, addlabels = TRUE, ncp=21)
for (i in 1:6) {
  print(fviz_contrib(pop.pca, choice = "var", axes = i))
}
```

## Supervised Learning

### Binary Classes
```{r}
set.seed(1996) ## for replication purpose
#split the data set in training/test set
index.tr <-
  createDataPartition(y = final_df$share_label,
                      p = 0.8, #trainset = 80% of the total dataset
                      list = FALSE)
final_df.tr <- final_df[index.tr,]
final_df.te <- final_df[-index.tr,]
```

```{r}
trControl <- trainControl(method  = "cv",
                          number  = 5) #5 fold Cross-Validation
```

```{r}
###KNN 
fit <- train(share_label ~ .,
             #trying to predict share_label with all other features
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10), #tuning K hyperparameters
             trControl  = trControl,
             metric     = "Accuracy",
             data       = final_df)
#k=9 maximizes our Kappa & Accuracy

final_df.knn <- knn3(data = final_df.tr,
                     share_label ~.,
                     k = 9)

final_df.knn.pred <- predict(final_df.knn,
                             newdata = final_df.te,
                             type = "class")

confusionMatrix(data = as.factor(final_df.knn.pred),
                reference = final_df.te$share_label)
```


```{r}
#as our RAM/CPU struggles while hyper-parameter tuning, we've chosen to sample a part of our data in order to have less computational effort
sub_hyp <- sample(nrow(final_df), 5000)
sub_hyp <- final_df[sub_hyp,]
```

```{r}
#SVM
hyperparameter_svm <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = seq(0,1,0.25))
svm_hyperpara_train <- train(share_label ~ .,
                         data = sub_hyp,
                         method = "svmRadial",
                         #radial instead of linear since
                         #we assume no linear relationship
                         trControl = trControl,
                         tuneGrid = hyperparameter_svm)
#The final values used for the model were sigma = 0.02 and C = 0.75.


final_df.svm_radial <- #train our radial kernel with sigma = 0.02 and C = 0.75
  svm(share_label ~ .,
      data = final_df.tr,
      kernel = "radial",
      tuneGrid = expand.grid(C = 0.75,
                             sigma = 0.02))

final_df.svm.pred_radial <- predict(final_df.svm_radial,
                                    newdata = final_df.te)

confusionMatrix(data = as.factor(final_df.svm.pred_radial),
                reference = final_df.te$share_label)
```

```{r}
#NAIVE BAYES without hyperparameter tuning
#since it only achieve a 0.177, we think that other algo can do better
final_df.nb <- naive_bayes(share_label ~ .,
                           data = final_df.tr,
                           kernel = TRUE)

final_df.nb.pred <- predict(final_df.nb,
                            newdata = final_df.te)

confusionMatrix(data = as.factor(final_df.nb.pred),
                reference = final_df.te$share_label)
```

```{r}
final_df.tree <- rpart(share_label ~ .,
                       data = final_df.tr)
rpart.plot(final_df.tree)

final_df.tree.pred <- predict(final_df.tree,
                              newdata = final_df.te,
                              type = "class")

confusionMatrix(data = as.factor(final_df.tree.pred),
                reference = final_df.te$share_label)

#Trying if pruning is needed
plotcp(final_df.tree)
#No need to prune
```

```{r}
# HYPERPARAMETER TUNING FOR RF
# THIS CODE WORD PERFECTLY BUT TAKES 20 MIN TO RUN
# WE GIVE YOU THE OUTPUT OF THIS RUN
# IN ORDER TO DECREASE KNIT TIME, WE REMOVE THE CODE AND CONSIDER IT AS TEST



#hyperparameter_rf <-
#  train(
#    share_label ~ .,
#    data = sub_hyp,
#    method = "rf",
#    metric = "Accuracy",
#    tuneGrid = expand.grid(.mtry = seq(1,5, 0.5)),
#    trControl = trControl
#  )

#Random Forest 

#10000 samples
#   21 predictor
#    2 classes: 'Unpopular', 'Popular' 

#No pre-processing
#Resampling: Cross-Validated (5 fold) 
#Summary of sample sizes: 8000, 7999, 8001, 8000, 8000 
#Resampling results across tuning parameters:

#  mtry  Accuracy  Kappa
#  1.0   0.626     0.247
#  1.5   0.639     0.275
#  2.0   0.639     0.277
#  2.5   0.641     0.280
#  3.0   0.633     0.265
#  3.5   0.636     0.271
#  4.0   0.633     0.266
#  4.5   0.634     0.266
#  5.0   0.634     0.268

#Accuracy was used to select the optimal model using the largest value.
#The final value used for the model was mtry = 2.5.
```


```{r}
rf <- train(share_label~.,
            data = final_df.tr,
            method = "rf",
            metric = "Accuracy",
            tuneGrid = expand.grid(.mtry = 2.5),
            #use the hyperparameter that we've find in
            #the hyperparameter tuning (chunk code line 153-195)
            trControl = trControl)

rf.te.pred <- predict(rf,
                      newdata = final_df.te,
                      type = "raw")

confusionMatrix(data = as.factor(rf.te.pred),
                reference = final_df.te$share_label)
# Best Kappa so far: 0.274
# Best Balanced Accuracy so far : 0.637 
```

```{r}
# Variable importance random forest
rf <- randomForest(share_label ~ ., data = final_df.tr)

varImpPlot(rf)

# some minutes to run but it works

# Gini coefficient is an impurity measure. The interpretation of the plot is the following: on the top the most important variables that contribute the most to decrease the node impurity. self_reference.avg_sharess is the most important variable and "day_is_weekend" is the less important.

# variable importance CART
# without shuffle 
final_df.tree <- rpart(share_label ~ ., data=final_df.tr)
rpart.plot(final_df.tree)
final_df.tree.pred <- predict(final_df.tree, newdata = final_df.te, type = "class")
confusionMatrix(data = as.factor(final_df.tree.pred),
                reference = final_df.te$share_label)

#shuffle on self_reference_avg_sharess

set.seed(143)

final_df.te_shuffle <- final_df.te
final_df.te_shuffle$self_reference_avg_sharess <- sample(final_df.te_shuffle$self_reference_avg_sharess)

final_df.tree.pre_shuffle <- predict(final_df.tree, newdata = final_df.te_shuffle, type = "class")

confusionMatrix(data = as.factor(final_df.tree.pre_shuffle), reference = final_df.te$share_label)

# Looking at the output we see that the accuracy change from 0.613 to 0.566 ( the latter is in the shuffle procedure)
```


### Multi-Classes : Searching for outstanding popularity
```{r}
set.seed(1996) ## for replication purpose
#split the data set in training/test set
c.index.tr <- createDataPartition(y = classes_df$share_classes,
                                  p = 0.8,
                                  list = FALSE)
classes_df.tr <- classes_df[c.index.tr,]
classes_df.te <- classes_df[-c.index.tr,]
```

```{r}
df.tr.unpop <- filter(classes_df.tr, share_classes == "Unpopular")
df.tr.pop <- filter(classes_df.tr, share_classes == "Popular")
df.tr.out <- filter(classes_df.tr, share_classes == "Outstanding")

n.out <- min(table(classes_df.tr$share_classes)) #1582

index.unpop <- sample(size=n.out, x=1:nrow(df.tr.unpop), replace=FALSE) ## sub-sample 840 instances from the "No"
index.pop <- sample(size=n.out, x=1:nrow(df.tr.pop), replace=FALSE) ## sub-sample 840 instances from the "No"

df.tr.subs <- data.frame(rbind(df.tr.out,
                               df.tr.unpop[index.unpop, ],
                               df.tr.pop[index.pop, ]))
table(df.tr.subs$share_classes)
```

```{r}
rf_outstanding <- train(share_classes~.,
            data = df.tr.subs,
            method = "rf",
            metric = "Accuracy",
            tuneGrid = expand.grid(.mtry = 2.5),
            #use the hyperparameter that we've find in
            #the hyperparameter tuning (chunk code line 153-195)
            trControl = trControl)

rf.te.pred <- predict(rf_outstanding,
                      newdata = classes_df.te,
                      type = "raw")

confusionMatrix(data = as.factor(rf.te.pred),
                reference = classes_df.te$share_classes)
# Poor Kappa: 0.194
```



