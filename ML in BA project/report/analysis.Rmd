# Section 4 : Analysis


```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```


## 4.1 : Unsupervised Learning

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

We start our unsupervised learning analysis by giving an overlook to the correlation between variables. It is important to look at the correlation structure in order to get a primary view of what probably going to happen for dimension reduction.

</div>

<br>

```{r}
ggcorrplot(cor(final_df[1:16]))
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Color gradation gives us the information about the positive/negative correlation intensity.
We can notice that there are some highly negative correlated variables, such as rate_negative_words and rate_positive_words and n_unique_token together with n_token_content.We conclude that these variables are similar enough that we could remove one of them or let the dimension reduction replace this variable. 
Rate_positive_words and global_sentiment are polarity positively correlated variables. For the rest of the variables, we see that it is difficult to see any strong correlation. Dimension reduction should provide us with a clearer picture.

</div>

<br>

### 4.1.1 : Principal Component Analysis (PCA)

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

After inspecting the correlation structure, we compute the Principal Component Analysis.
In order to give the same importance to each variable, we scale the data first, and then compute the PCA.

</div>

<br>

```{r}
summary(prcomp(final_df[1:16], scale = TRUE))
pop.pca <- PCA(final_df[1:16], ncp = 21, graph = FALSE)
```

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

* First component:
Mainly positively correlated with n_non_stop_unique_token and n_unique_token and slightly positively correlated with rate_positive_words; negatively correlated with n_token_content, slightly negatively correlated with rate_negative_words.

* Second component:
Mainly positively correlated with rate_positive_words and global_sentiment_polarity; negatively correlated with rate_negative_words. Slightly negatively correlated also with n_non_stop_unique_token and n_unique_token.

What we conclude from this graph is that there are some highly correlated variables. We are not going to remove them and instead let dimension reduction do its job.

</div>

<br>

```{r}
fviz_pca_var(pop.pca)
```

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Notice that the first dimension accounts for 18.8% of the total variance, and the second dimension for 17.6%. For the third component the explained variance is halved with respect to the second component. To have 60% of the total variance explained, we would need to reduce the dimensions with 5 components. 5 components seems like a lo, but if they make sense (that the components include distinct features), this number of dimensions is justified. Below we can see the composition of each of the first 4 components.
</div>

<br>

```{r}
fviz_eig(pop.pca, addlabels = TRUE, ncp = 21)
```

<br>


```{r}
for (i in 1:4) {
  print(fviz_contrib(pop.pca, choice = "var", axes = i))
}
```

<br>

## 4.2 : Supervised Learning

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

In this section, we will fit different models in order to attempt to predict the share popularity of a news article.
We conducted two supervised learning tasks: a binary class one and a multi class one.
As mentioned earlier, we have chosen to label the instances with below a share value below the median as Unpopular and instances above the median as Popular. Our goal with this binary classification task, is to predict whether an article will be popular or not.
For the multi-class case we create another category for the top 5% in terms of popularity: Outstanding. The aim with this classification is to see whether our best models are able to predict particularly popular articles.

</div>

<br>

### 4.2.1 : Binary Classes

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

As always, we split the dataset into 2 subgroups: the training set and the test set. We decided to make a training set that accounts for 80% of the dataset, and the test set is the remaining 20%.

</div>

<br>

```{r warning=FALSE}
set.seed(1996) ## for replication purpose
#split the data set in training/test set
index.tr <-
  createDataPartition(y = final_df$share_label,
                      p = 0.8, #trainset = 80% of the total dataset
                      list = FALSE)
final_df.tr <- final_df[index.tr,]
final_df.te <- final_df[-index.tr,]
print(head(final_df.tr))
print(head(final_df.te))
```

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

With the goal of avoid overfitting and selection bias, we will also perform a cross validation split. We have decided to use only a 5-fold cross-validation in order to minimize computational time, because our dataset is very large. 

</div>

<br>

```{r}
trControl <- trainControl(method  = "cv",
                          number  = 5) #5 fold Cross-Validation
```

#### 4.2.1.1 : K-nearest neighbor (KNN)

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

The first algorithm that we are going to test on our dataset is the KNN.
We're going to use all the selected features in order to check the prediction abilities of KNN on this dataset.
It is important to decide which value of the hyperparameter K the model should use in order to optimize the prediction power of the algorithm and our results. To do this we tuned the hyperparameter K. We defined range of values that K could assume: from 1 to 10. 

From the hyperparameter tuning results, it turns out that the value of K = 9 maximizes the Kappa and the accuracy. We'll fit our training set with this hyperparameter setting.

</div>


```{r warning=FALSE}
###KNN 
#trying to predict share_label with all other features
#fit <- train(share_label ~ .,
#             method     = "knn",
#             tuneGrid   = expand.grid(k = 8:20), #tuning K hyperparameters
#             trControl  = trControl,
#             metric     = "Accuracy", 
#             data       = final_df)
#we put it as text in order to decrease knit time: you can remove the # 

#k=9 maximizes our Kappa & Accuracy
final_df.knn <- knn3(data = final_df.tr,
                     share_label ~.,
                     k = 19)
final_df.knn.pred <- predict(final_df.knn,
                             newdata = final_df.te,
                             type = "class")
confusionMatrix(data = as.factor(final_df.knn.pred),
                reference = final_df.te$share_label)
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

As we can see from the results, the prediction quality of the KNN algorithm is not satisfying, we achieve a Balanced Accuracy of 0.558. This algorithm also has more difficulty in predicting popular articles: Specificity < Sensitivity. 
The Cohen's Kappa score compares the accuracy that we found with the algorithm with the accuracy that would have been achieved with essentially random predictions. If it is close to 1, it means that our algorithm is performing well. We see that here, the Kappa is close to 0.116, this algorithm is almost indistinguishable from an algorithm that would predict randomly based on knowing the proportions of each category.


</div>


<br>

#### 4.2.1.2 : Naive Bayes

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

The next prediction algorithm we will train on our dataset is the Naive Bayes classifier.
As always, we train the model on the training set and compute the prediction on the test set. If you go back to the section where we show visualisations of each of our features, you can see that few of them have a normal distribution. So we chose to train the model with a kernel density estimator, which is more flexible in terms of the shape of the distribution.

</div>


```{r}
#NAIVE BAYES without hyperparameter tuning
#since it only achieve a 0.177, we think that other algorithms can do better
final_df.nb <- naive_bayes(share_label ~ .,
                           data = final_df.tr,
                           kernel = TRUE)
final_df.nb.pred <- predict(final_df.nb,
                            newdata = final_df.te)
confusionMatrix(data = as.factor(final_df.nb.pred),
                reference = final_df.te$share_label)
```

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

We notice that this algorithm works better than the previous one. It predicts 0.761 of unpopular articles correctly. Unfortunately, it has a very poor ability to predict popular articles (Specificity : 0.438). Thus, the prediction quality of the  algorithm is still not satisfying, we achieve a Balanced Accuracy of 0.600. With this method, we achieved a Kappa of .195, which is still very low. 

</div>

<br>



##### Sub-sampling : hyper-parameter tuning issue

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

As our RAM/CPU struggles with hyper-parameter tuning, we've chosen to sample a part of our data in order to have minimize the required computational power. 

```{r}
#as our RAM/CPU struggles while hyper-parameter tuning, we've chosen to sample a part of our data in order to have less computational effort
sub_hyp <- sample(nrow(final_df), 5000)
sub_hyp <- final_df[sub_hyp,]

print("sub-sample")
datatable(
  sub_hyp,
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX = T)
)

```

</div>

#### 4.2.1.3 : Support Vector Machine (SVM)

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

The next algorithm for which we wanted to check the prediction accuracy for our dataset is SVM.
As we have a lot of variables, correlated features, and non-normal distribution, we assume a non-linear relationship between variables. We thus decided to use a radial kernel for our analysis.

T select our hyperparameters we decided to perform a grid-search based on 8 values.

| Sigma | Cost | 
|------------------|------------------|
| 0.01, 0.02, 0.05, 0.1 | 0.00, 0.25, 0.50, 0.75, 1.00 |


The values of sigma and C that maximize our scores and that we're going to use for the model training are: sigma = 0.01 and C = 0.25

</div>


```{r warning=FALSE}
#SVM
hyperparameter_svm <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = seq(0,1,0.25))
svm_hyperpara_train <- train(share_label ~ .,
                         data = sub_hyp,
                         method = "svmRadial",
                         #radial instead of linear since
                         #we assume no linear relationship
                         trControl = trControl,
                         tuneGrid = hyperparameter_svm)
svm_hyperpara_train

#The final values used for the model were sigma = 0.01 and C = 0.25.

final_df.svm.pred_radial <- predict(svm_hyperpara_train,
                                    newdata = final_df.te)
confusionMatrix(data = as.factor(final_df.svm.pred_radial),
                reference = final_df.te$share_label)
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

This algorithm gives us the best results so far. The Kappa is equal to 0.264 and the Balanced Accuracy  is equal to 0.631. This is still however not very good. Even with this algorithm, our model has trouble classifying articles correctly. If we look at the Sensitivity score (=0.558), we can see that it struggles predicting unpopular articles. If we look at the  Specificity score (=0.705), we're reaching a decent level of prediction for popular articles.

</div>

<br>

#### 4.2.1.4 :Classification and regression tree (CART)

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

The second to last algorithm for which we wanted to check the prediction accuracy is the CART algorithm.
This algorithm can also be interesting not only for predictive purposes but also to see which features most influence the classification process.

</div>

```{r}
final_df.tree <- rpart(share_label ~ .,
                       data = final_df.tr)
rpart.plot(final_df.tree)
final_df.tree.pred <- predict(final_df.tree,
                              newdata = final_df.te,
                              type = "class")
confusionMatrix(data = as.factor(final_df.tree.pred),
                reference = final_df.te$share_label)
```
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

The scores for this model are worse than those of the SVM model, we will not keep this model to predict our new instances.

But it is very interesting to note which features influence the classification. As we have already seen in the EDA part, we can see that most of the choices are made on the theme of the article, the day and the number of referrals. If the topic of the article is something other than Business/Entertainment/World (i.e. Lifestyle or Social Media) or if it is published on the weekend, it is more likely to be popular. It is worth noting that the NLP values are not the most important for the classification choice .

To very that this algorithm isn't overfitiing, we will check if the tree does needs to be pruned : 

</div>

```{r}
#Trying if pruning is needed
plotcp(final_df.tree)
#No need to prune
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

There is no overfitting signal, no need to prune the tree. This is understandable given the low scores we acheived.

</div>

<br>

#### 4.2.1.5 : Random Forest

```{r}
# HYPERPARAMETER TUNING FOR RF
# THIS CODE WORK PERFECTLY BUT TAKES 20 MIN TO RUN
# WE GIVE YOU THE OUTPUT OF THIS RUN
# IN ORDER TO DECREASE KNIT TIME, WE REMOVE THE CODE.
# -> if you want to verify our score, please remove the #


#hyperparameter_rf <-
#  train(
#    share_label ~ .,
#    data = sub_hyp,
#    method = "rf",
#    metric = "Accuracy",
#    tuneGrid = expand.grid(.mtry = seq(1,5, 0.5)),
#    trControl = trControl
#  )




#Random Forest 

#5000 samples
#  19 predictor
#   2 classes: 'Unpopular', 'Popular' 

#No pre-processing
#Resampling: Cross-Validated (5 fold) 
#Summary of sample sizes: 4000, 4000, 4000, 4000, 4000 
#Resampling results across tuning parameters:

#  mtry  Accuracy  Kappa
#  1.0   0.628     0.236
#  1.5   0.639     0.268
#  2.0   0.640     0.271
#  2.5   0.642     0.274
#  3.0   0.640     0.271
#  3.5   0.640     0.271
#  4.0   0.639     0.270
#  4.5   0.634     0.259
#  5.0   0.633     0.257

#Accuracy was used to select the optimal model using the largest value.
#The final value used for the model was mtry = 2.5.
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

We wanted to test the predictive capabilities of this model because the literature defines it as the best for predicting this dataset. The only hyperparameter that we've tuned with this algorithm is the number of random variables selected by the algorithm when forming each tree division (mtry). The mtry which maximises our scores is 2.5. We will therefore perform a random forest with mtry = 2.5 and see what its predictive power is.

</div>


```{r warning=FALSE}
rf <- train(share_label~.,
            data = final_df.tr,
            method = "rf",
            metric = "Accuracy",
            tuneGrid = expand.grid(.mtry = 2.5),
            #use the hyperparameter that we've find in
            #the hyperparameter tuning (chunk code line 153-195)
            trControl = trControl)
rf.te.pred <- predict(rf,
                      newdata = final_df.te,
                      type = "raw")
confusionMatrix(data = as.factor(rf.te.pred),
                reference = final_df.te$share_label)
# Best Kappa so far: 0.289
# Best Balanced Accuracy so far : 0.644
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

The random forest model gives us very similar results to the SMV model but slightly better. We can reach a 0.289 Kappa and a 0.644 Balanced Accuracy. This algorithm gives us the best results. It is this algorithm that should be used to plan new instances.
We note, however, that these scores are still not very high and that our algorithms have difficulty predicting which articles are popular or not. We have about a 65% chance of predicting the correct classification. Which is rather low. We will discuss why we think these algorithms perform so poorly later on. 

</div>

<br>

####  Variable importance

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

As the random forest is our best model, we found it important to mention which features were the most important for theclassification task. For this we have decided to focus on the mean decrease Gini. This score expresses how much accuracy the model loses if we remove a particular feature. The higher the MDG, the more important the feature.

</div>

<br>

```{r}
# Variable importance random forest
rf <- randomForest(share_label ~ ., data = final_df.tr)
varImpPlot(rf)
```

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

On the top of this plot, we can see the variables that contribute the most to decrease the node impurity. To sum up: self_reference.avg_shares is the most important variable and "day_is_weekend" is the least important in order to classify articles. That day_is_weekend is important is not surprising as the day information is already in another feature, so we could have removed it to reduce the complexity of the model.
On the other hand, what is more surprising is that the number of images and the number of videos is not that important, whereas we would intuitively consider this information to be more useful. What seems to be more important is the number of references, the average length of the article and the level of subjectivity of the article.

</div>

```{r}
# variable importance CART

# without shuffle 
#final_df.tree <- rpart(share_label ~ ., data=final_df.tr)
#rpart.plot(final_df.tree)
#final_df.tree.pred <- predict(final_df.tree, newdata = final_df.te, type = "class")
#confusionMatrix(data = as.factor(final_df.tree.pred),
#                reference = final_df.te$share_label)
#
#
##shuffle on self_reference_avg_sharess
#set.seed(143)
#final_df.te_shuffle <- final_df.te
#final_df.te_shuffle$self_reference_avg_sharess <- sample(final_df.te_shuffle$self_reference_avg_sharess)
#final_df.tree.pre_shuffle <- predict(final_df.tree, newdata = final_df.te_shuffle, type = "class")
#confusionMatrix(data = as.factor(final_df.tree.pre_shuffle), reference = final_df.te$share_label)
#Looking at the output we see that the accuracy changes from 0.613 to 0.566 ( the latter result is in the shuffle procedure).
```
<br>


### 4.2.2 : Multi-Classes : Searching for outstanding popularity

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

What we presented earlier is a binary classification of whether an article is popular or not. This analysis is done in most of the literature on this dataset. However, we also wanted to analyse whether it was possible to predict the most shared articles (by this we mean articles for which the share value is higher than the 0.95 quantile). In this new part, we want to check if our best algorithm can predict the articles with impressive and rare popularity.

</div>

```{r}
set.seed(1996) ## for replication purpose
#split the data set in training/test set
c.index.tr <- createDataPartition(y = classes_df$share_classes,
                                  p = 0.8,
                                  list = FALSE)
classes_df.tr <- classes_df[c.index.tr,]
classes_df.te <- classes_df[-c.index.tr,]
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

To do such an analysis, we had to change how our instances were distributed in classes because our classes were not proportional.

</div>

```{r}
df.tr.unpop <- filter(classes_df.tr, share_classes == "Unpopular")
df.tr.pop <- filter(classes_df.tr, share_classes == "Popular")
df.tr.out <- filter(classes_df.tr, share_classes == "Outstanding")
n.out <- min(table(classes_df.tr$share_classes)) #1582
index.unpop <- sample(size=n.out, x=1:nrow(df.tr.unpop), replace=FALSE) ## sub-sample 840 instances from the "No"
index.pop <- sample(size=n.out, x=1:nrow(df.tr.pop), replace=FALSE) ## sub-sample 840 instances from the "No"
df.tr.subs <- data.frame(rbind(df.tr.out,
                               df.tr.unpop[index.unpop, ],
                               df.tr.pop[index.pop, ]))
table(df.tr.subs$share_classes)
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Now that we have a balanced training set, we can start our analysis.

</div>

<br>


##### Random forest

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

In an attempt to predict articles with outstanding popularity, we chose the model that worked best in our binary analysis. In the following part, we computed a random forest on our multi-class variable.

</div>

```{r}
rf_outstanding <- train(share_classes~.,
            data = df.tr.subs,
            method = "rf",
            metric = "Accuracy",
            tuneGrid = expand.grid(.mtry = 2.5),
            #use the hyperparameter that we've find in
            #the hyperparameter tuning (chunk code line 153-195)
            trControl = trControl)

rf.te.pred <- predict(rf_outstanding,
                      newdata = classes_df.te,
                      type = "raw")

confusionMatrix(data = as.factor(rf.te.pred),
                reference = classes_df.te$share_classes)

```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

This algorithm gives a good specificity for the three classes. For the outstanding class, the algorithm manages to recognize 208/367 instances. We find that this is a decent performance considering our results in the binary classification task. However, the Kappa we've found is equal to 0.209, which is not very high.


</div>

```{r}
# Variable importance random forest
rf <- randomForest(share_classes ~ ., data = df.tr.subs)
varImpPlot(rf)
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Very little change in the importance of variables compared to the binary analysis. The interpretation made earlier can also be applied here.

</div>
