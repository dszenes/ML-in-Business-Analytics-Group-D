# Section 2: Data description

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```


## Raw Data Set


## 2.1 Mashable Data Set : Online News Popularity

```{r}
df <- read_csv(file = here::here("data/OnlineNewsPopularity.csv"))
df <- as_tibble(df)
```

```{r}
df2 <- read.csv(file = here::here("data/OnlineNewsPopularity.csv"))
```



_Source of the data set:_ [https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity]

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

UCI Data Set Information:

* _This dataset is a collection of 39'644 articles published on Mashable, the largest independent news source dedicated to digital culture, social media and technology. A text mining analysis was carried out on each of the articles. Each of the articles obtained various basic statistics from a text mining analysis (the number of words, the general feeling of the article, the theme of the article, ...). The dataset also takes into account the internal details of the document (how many images/videos are in the article, which topic is classified, which day it is classified,...)_

* Acquisition date: January 8, 2015

```{r}
datatable(
  df[1:50, ],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX = T)
)
```

<br>

</div>



```{r}
summary_df <- summary(df)
summary_df %>% kable_maker(caption = "summary_df")
```

<br>

```{r, fig.height=8, fig.width=12, fig.fullwidth=TRUE, out.width = '100%', fig.align='center'}
par(mfrow = c(4, 6))
for (i in 2:length(df2)) {
  hist(df2[, i], xlab = names(df2)[i])
  boxplot(df2[, i], xlab = names(df2)[i])
}
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">


In this dataset, each instance is an article on Mashable. The number of shares, our variable of interest, is given to us in the form for of a simple numerical value. As you can see on the last histogram/boxplot, the feature that we want to predict is not normally distributed. To get rid of that, we've to log-transform this feature. We have also decided in this case to transform this variable into a categorical variable with three levels: Unpopular, Popular and Outstanding. These dividing lines were based on the distribution of the number of shares, namely the median point which determines Unpopular articles, while Outstanding articles are in the top 5 %. 

Compared to previous research on this dataset [1,2], we have decided to limit the number of variables we use to the ones that we think are the most interpretable and thus create a model that is can be more easily generalized and used by other companies that want to promote their online content. We have, for instance, decided not to use the results of a topic modelling analysis called the Latent Dirichlet allocation (LDA), because this is a rather complicated technique which is difficult to interpret to people not familiar with natural language processing. We have also decided to eliminate some highly intercorrelated variables, such as the minimum and maximum of the shares of other Mashable articles referenced in the content of the current instance, keeping only the average value for this variable
.

Finally, observing the very long tail of n_tokens_content, we performed some manual verification using the article links. We find that there are quite a few erroneous entries for this variable at the extreme ends of the distribution. Namely, the articles with 0 words actually do have text, but the web scraping technique used by the creators of the dataset seems not to notice text that is above the share button in the article.There are also some very high word counts above 5000 words. For every manual check in this category we found that the article wasn't actually this long. We therefore decided to limit our analysis to articles with word counts higher than 0, but lower than 1500. We find this to be a realistic range for these articles, and this will perhaps eliminate erroneous intances.

</div>

<br>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

* The predictive variables that we have decided to keep in their original form can be sorted into a few categories :

Numeric integer values:

  + `n_tokens_title` Number of words in the title
  + `n_tokens_content` Number of words in the content
  + `num_hrefs` Number of links
  + `num_imgs` Number of images
  + `num_videos` Number of videos
  + `num_keywords` Number of keywords (linked subjects that define the article) in the metadata
  + `self_reference_avg_sharess` Avg. shares of referenced articles in Mashable (rounded)

Numeric real values:

  + `average_token_length` Average length of the words in the content

Rates, numerical scores between [0:1] :

  + `n_unique_tokens` Rate of unique words in the content
  + `n_non_stop_unique_tokens` Rate of unique non-stop words in the content
  + `global_subjectivity` Text subjectivity
  + `rate_positive_words` Rate of positive words among non-neutral tokens
  + `rate_negative_words` Rate of negative words among non-neutral tokens
  + `title_subjectivity` Title subjectivity

Rates, numerical scores between [-1:1] :

  + `global_sentiment_polarity` Text sentiment polarity
  + `title_sentiment_polarity` Title polarity

As their name implies, subjectivity and polarity are NLP techniques that measure levels of subjective opinions and negative/positive emotions expressed respectively, which is why they have different scales. Rates of negative and positive words are measured among the tokens that express emotion (non-neutral), and thus add up to one.

In addition, there are some dummy variables that we have changed into categorical levels of different levels in order to reduce the number of predictors:

  + `data_channel`  6 levels (Lifestyle, Entertainment, Business, Social Media, Tech, World)
  + `day`  7 levels (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday Sunday)
  + `day_is_weekend ` 2 levels(yes,no)

</div>

<br>

```{r}
zero_df <- df %>%
  filter(n_tokens_content == 0)
```

```{r}
non_zero_df <- df %>%
  filter(n_tokens_content > 0)
```

```{r}
long_df <- df %>%
  filter(n_tokens_content > 700)
```

```{r}
normal_df <- df %>%
  filter(n_tokens_content < 1500 & n_tokens_content > 0)
normal_df <- normal_df %>% mutate(shares = log(shares))
```

```{r}
#creating shares popularity classes based on quantiles
#quantile(normal_df$shares, probs = c(seq(0,1, length.out = 5)))
#quantile(normal_df$shares, probs = c(seq(0,1, length.out = 50)))
share_label <- cut(normal_df$shares, breaks = c(0.00, 7.24, 13.65))
levels(share_label) <- c("Unpopular", "Popular")
new <- tibble(normal_df, share_label)
```

```{r}
new$data_channel <- rep("Lifestyle", nrow(new))
new$data_channel[new$data_channel_is_entertainment == 1] <-"Entertainment"
new$data_channel[new$data_channel_is_bus == 1] <- "Business"
new$data_channel[new$data_channel_is_socmed == 1] <- "Social Media"
new$data_channel[new$data_channel_is_tech == 1] <- "Tech"
new$data_channel[new$data_channel_is_world == 1] <- "World"

new$data_channel <- as.factor(new$data_channel)
```

```{r}
new$day <- rep("Monday", nrow(new))
new$day[new$weekday_is_tuesday == 1] <- "Tuesday"
new$day[new$weekday_is_wednesday == 1] <- "Wednesday"
new$day[new$weekday_is_thursday == 1] <- "Thursday"
new$day[new$weekday_is_friday == 1] <- "Friday"
new$day[new$weekday_is_saturday == 1] <- "Saturday"
new$day[new$weekday_is_sunday == 1] <- "Sunday"

new$day <- as.factor(new$day)
```

```{r}
#looking at the publishing day of the article
new <- new %>%
  mutate(weekend=as.factor(is_weekend)) %>%
  mutate(day_is_weekend=fct_collapse(weekend,"yes"= "1","no"= "0"))
```

```{r}
final_df <-
  new %>% select(
    -url,
    -timedelta,
    -n_non_stop_words,
    -data_channel_is_lifestyle,
    -data_channel_is_entertainment,
    -data_channel_is_bus,
    -data_channel_is_socmed,
    -data_channel_is_tech,
    -data_channel_is_world,
    -weekday_is_monday,
    -weekday_is_tuesday,
    -weekday_is_wednesday,
    -weekday_is_thursday,
    -weekday_is_friday,
    -weekday_is_saturday,
    -weekday_is_sunday,
    -is_weekend,
    -kw_min_min,
    -kw_max_min,
    -kw_avg_min,
    -kw_min_max,
    -kw_max_max,
    -kw_avg_max,
    -kw_min_avg,
    -kw_max_avg,
    -kw_avg_avg,
    -LDA_00,
    -LDA_01,
    -LDA_02,
    -LDA_03,
    -LDA_04,
    -self_reference_min_shares,
    -self_reference_max_shares,
    -min_positive_polarity,
    -max_positive_polarity,
    -min_negative_polarity,
    -max_negative_polarity,
    -num_self_hrefs,
    -abs_title_subjectivity,
    -abs_title_sentiment_polarity,
    -avg_positive_polarity,
    -avg_negative_polarity,
    -global_rate_negative_words,
    -global_rate_positive_words,
    -weekend,
    -shares
  ) %>% filter(!is.na(share_label))
```

```{r}
#we want create an extra category for an exceptionally high number of shares, we chose a number that will define the 5 % most popular articles
#quantile(normal_df$shares, probs = 0.95)
#quantile(normal_df$shares, probs = 0.5)
```

```{r}
#creating shares popularity classes based on quantiles
share_classes <- cut(normal_df$shares, breaks = c(0.00, 7.24, 9.27, 13.65))
levels(share_classes) <- c("Unpopular", "Popular", "Outstanding")
new2 <- tibble(normal_df, share_classes)
```

```{r}
new2$data_channel <- rep("Lifestyle", nrow(new2))
new2$data_channel[new2$data_channel_is_entertainment == 1] <- "Entertainment"
new2$data_channel[new2$data_channel_is_bus == 1] <- "Business"
new2$data_channel[new2$data_channel_is_socmed == 1] <- "Social Media"
new2$data_channel[new2$data_channel_is_tech == 1] <- "Tech"
new2$data_channel[new2$data_channel_is_world == 1] <- "World"

new2$data_channel <- as.factor(new2$data_channel)
```

```{r}
new2$day <- rep("Monday", nrow(new2))
new2$day[new2$weekday_is_tuesday == 1] <- "Tuesday"
new2$day[new2$weekday_is_wednesday == 1] <- "Wednesday"
new2$day[new2$weekday_is_thursday == 1] <- "Thursday"
new2$day[new2$weekday_is_friday == 1] <- "Friday"
new2$day[new2$weekday_is_saturday == 1] <- "Saturday"
new2$day[new2$weekday_is_sunday == 1] <- "Sunday"

new2$day <- as.factor(new2$day)
```

```{r}
#looking at the publishing day of the article
new2 <- new2 %>%
  mutate(weekend=as.factor(is_weekend)) %>%
  mutate(day_is_weekend=fct_collapse(weekend,"yes"= "1","no"= "0"))
```

```{r}
classes_df <-
  new2 %>% select(
    -url,
    -timedelta,
    -is_weekend,
    -n_non_stop_words,
    -data_channel_is_lifestyle,
    -data_channel_is_entertainment,
    -data_channel_is_bus,
    -data_channel_is_socmed,
    -data_channel_is_tech,
    -data_channel_is_world,
    -weekday_is_monday,
    -weekday_is_tuesday,
    -weekday_is_wednesday,
    -weekday_is_thursday,
    -weekday_is_friday,
    -weekday_is_saturday,
    -weekday_is_sunday,
    -kw_min_min,
    -kw_max_min,
    -kw_avg_min,
    -kw_min_max,
    -kw_max_max,
    -kw_avg_max,
    -kw_min_avg,
    -kw_max_avg,
    -kw_avg_avg,
    -LDA_00,
    -LDA_01,
    -LDA_02,
    -LDA_03,
    -LDA_04,
    -self_reference_min_shares,
    -self_reference_max_shares,
    -min_positive_polarity,
    -max_positive_polarity,
    -min_negative_polarity,
    -max_negative_polarity,
    -num_self_hrefs,
    -abs_title_subjectivity,
    -abs_title_sentiment_polarity,
    -avg_positive_polarity,
    -avg_negative_polarity,
    -global_rate_negative_words,
    -global_rate_positive_words,
    -weekend,
    -shares
  ) %>% filter(!is.na(share_classes))
```
