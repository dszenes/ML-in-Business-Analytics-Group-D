# 2 Data

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

## Raw Data Set

<br>

## 2.1 Mashable Data Set

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

First, we will load the data set that record some sets of features about Mashable articles collected in a period of two years.

</div>

<br>

### Online News Populartiy
```{r}
df <- read_csv(file = here::here("data/OnlineNewsPopularity.csv"))
df <- as_tibble(df)
```

```{r}
df2 <- read.csv(file = here::here("data/OnlineNewsPopularity.csv"))
```

<br>


_Source of the data set:_ [https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity]

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Data Set Information:

* This dataset is a collection of 39'644 articles published on Mashable, the largest independent news source dedicated to digital culture, social media and technology. A text mining analysis was carried out on each of the articles. Each of the articles obtained various basic statistics from a text mining analysis (the number of words, the general feeling of the article, the theme of the article, ...). The dataset also takes into account the internal details of the document (how many images/videos are in the article, which topic is classified, which day it is classified,...)

* Acquisition date: January 8, 2015

```{r}
datatable(
  df[1:50, ],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX = T)
)
```

<br>

</div>

<br>

* Find below the variables of the data set


  + `url` URL of the article (non-predictive)
  + `timedelta`  Days between the article publication and the dataset acquisition (non-predictive)
  + `n_tokens_title` Number of words in the title 
  + `n_tokens_content` Number of words in the content
  + `n_unique_tokens` Rate of unique words in the content
  + `n_non_stop_words` Rate of non-stop words in the content
  + `n_non_stop_unique_tokens` Rate of unique non-stop words in the content
  + `num_hrefs` Number of links 
  + `num_self_hrefs` Number of links to other articles published by Mashable
  + `num_imgs` Number of images
  + `num_videos` Number of videos
  + `average_token_length` Average length of the words in the content
  + `num_keywords` Number of keywords in the metadata 
  + `data_channel_is_lifestyle` Is data channel 'Lifestyle'?
  + `data_channel_is_entertainment` Is data channel 'Entertainment'?
  + `data_channel_is_bus` Is data channel 'Business'?
  + `data_channel_is_socmed` Is data channel 'Social Media'?
  + `data_channel_is_tech` Is data channel 'Tech'?
  + `data_channel_is_world` Is data channel 'World'?
  + `kw_min_min` Worst keyword (min. shares)
  + `kw_max_min` Worst keyword (max. shares)
  + `kw_avg_min` Worst keyword (avg. shares)
  + `kw_min_max` Best keyword (min. shares)
  + `kw_max_max` Best keyword (max. shares)
  + `kw_avg_max` Best keyword (avg. shares)
  + `kw_min_avg` Avg. keyword (min. shares)
  + `kw_max_avg` Avg. keyword (max. shares)
  + `kw_avg_avg` Avg. keyword (avg. shares)
  + `self_reference_min_shares` Min. shares of referenced articles in Mashable
  + `self_reference_max_shares` Max. shares of referenced articles in Mashable
  + `self_reference_avg_sharess` Avg. shares of referenced articles in Mashable
  + `weekday_is_monday` Was the article published on a Monday?
  + `weekday_is_tuesday` Was the article published on a Tuesday?
  + `weekday_is_wednesday` Was the article published on a Wednesday?
  + `weekday_is_thursday` Was the article published on a Thursday?
  + `weekday_is_friday` Was the article published on a Friday?
  + `weekday_is_saturday` Was the article published on a Saturday?
  + `weekday_is_sunday` Was the article published on a Sunday?
  + `is_weekend` Was the article published on the weekend?
  + `LDA_00` Closeness to LDA topic 0
  + `LDA_01` Closeness to LDA topic 1
  + `LDA_02` Closeness to LDA topic 2
  + `LDA_03` Closeness to LDA topic 3
  + `LDA_04` Closeness to LDA topic 4
  + `global_subjectivity` Text subjectivity
  + `global_sentiment_polarity` Text sentiment polarity
  + `global_rate_positive_words` Rate of positive words in the content
  + `global_rate_negative_words` Rate of negative words in the content
  + `rate_positive_words` Rate of positive words among non-neutral tokens
  + `rate_negative_words` Rate of negative words among non-neutral tokens
  + `avg_positive_polarity` Avg. polarity of positive words
  + `min_positive_polarity` Min. polarity of positive words
  + `max_positive_polarity` Max. polarity of positive words
  + `avg_negative_polarity` Avg. polarity of negative words
  + `min_negative_polarity` Min. polarity of negative words
  + `max_negative_polarity` Max. polarity of negative words
  + `title_subjectivity` Title subjectivity
  + `title_sentiment_polarity` Title polarity
  + `abs_title_subjectivity` Absolute subjectivity level
  + `abs_title_sentiment_polarity` Absolute polarity level
  + `shares` Number of shares (target)

<br>

```{r}
summary_df <- summary(df)
summary_df %>% kable_maker(caption = "summary_df")
```

<br>

```{r, fig.height=8, fig.width=12, fig.fullwidth=TRUE, out.width = '100%', fig.align='center'}
par(mfrow = c(4, 6))
for (i in 2:length(df2)) {
  hist(df2[, i], xlab = names(df2)[i])
  boxplot(df2[, i], xlab = names(df2)[i])
}
```


<br>


### Filtering the raw data set

```{r}
zero_df <- df %>%
  filter(n_tokens_content == 0)
```

```{r}
non_zero_df <- df %>%
  filter(n_tokens_content > 0)
```

```{r}
long_df <- df %>%
  filter(n_tokens_content > 700)
```

```{r}
normal_df <- df %>%
  filter(n_tokens_content < 1500 & n_tokens_content > 0)
```

```{r}
#creating shares popularity classes based on quantiles

quantile(normal_df$shares, probs = c(seq(0,1, length.out = 5)))
quantile(normal_df$shares, probs = c(seq(0,1, length.out = 50)))

share_label <- cut(normal_df$shares, breaks = c(0, 1400, 843300))
levels(share_label) <- c("Unpopular", "Popular")

new <- tibble(normal_df, share_label)
```

```{r}
new$data_channel <- rep("Lifestyle", nrow(new))
new$data_channel[new$data_channel_is_entertainment == 1] <-"Entertainment"
new$data_channel[new$data_channel_is_bus == 1] <- "Business"
new$data_channel[new$data_channel_is_socmed == 1] <- "Social Media"
new$data_channel[new$data_channel_is_tech == 1] <- "Tech"
new$data_channel[new$data_channel_is_world == 1] <- "World"

new$data_channel <- as.factor(new$data_channel)
```

```{r}
new$day <- rep("Monday", nrow(new))
new$day[new$weekday_is_tuesday == 1] <- "Tuesday"
new$day[new$weekday_is_wednesday == 1] <- "Wednesday"
new$day[new$weekday_is_thursday == 1] <- "Thursday"
new$day[new$weekday_is_friday == 1] <- "Friday"
new$day[new$weekday_is_saturday == 1] <- "Saturday"
new$day[new$weekday_is_sunday == 1] <- "Sunday"

new$day <- as.factor(new$day)
```

```{r}
#looking at the publishing day of the article
new <- new %>%
  mutate(weekend=as.factor(is_weekend)) %>%
  mutate(day_is_weekend=fct_collapse(weekend,"yes"= "1","no"= "0"))
```

```{r}
library(ggcorrplot)
ggcorrplot(cor(normal_df[2:ncol(normal_df)]))
```

```{r}
final_df <-
  new %>% select(
    -url,
    -timedelta,
    -data_channel_is_lifestyle,
    -data_channel_is_entertainment,
    -data_channel_is_bus,
    -data_channel_is_socmed,
    -data_channel_is_tech,
    -data_channel_is_world,
    -weekday_is_monday,
    -weekday_is_tuesday,
    -weekday_is_wednesday,
    -weekday_is_thursday,
    -weekday_is_friday,
    -weekday_is_saturday,
    -weekday_is_sunday,
    -kw_min_min,
    -kw_max_min,
    -kw_avg_min,
    -kw_min_max,
    -kw_max_max,
    -kw_avg_max,
    -kw_min_avg,
    -kw_max_avg,
    -kw_avg_avg,
    -LDA_00,
    -LDA_01,
    -LDA_02,
    -LDA_03,
    -LDA_04,
    -self_reference_min_shares,
    -self_reference_max_shares,
    -min_positive_polarity,
    -max_positive_polarity,
    -min_negative_polarity,
    -max_negative_polarity,
    -num_self_hrefs,
    -abs_title_subjectivity,
    -abs_title_sentiment_polarity,
    -avg_positive_polarity,
    -avg_negative_polarity,
    -global_rate_negative_words,
    -global_rate_positive_words,
    -weekend,
    -shares
  )
```

```{r}
#creating shares popularity classes based on quantiles
share_classes <- cut(normal_df$shares, breaks = c(0, 1400, 10000, 843300))
levels(share_classes) <- c("Unpopular", "Popular", "Outstanding")
new2 <- tibble(normal_df, share_classes)
```

```{r}
new2$data_channel <- rep("Lifestyle", nrow(new2))
new2$data_channel[new2$data_channel_is_entertainment == 1] <- "Entertainment"
new2$data_channel[new2$data_channel_is_bus == 1] <- "Business"
new2$data_channel[new2$data_channel_is_socmed == 1] <- "Social Media"
new2$data_channel[new2$data_channel_is_tech == 1] <- "Tech"
new2$data_channel[new2$data_channel_is_world == 1] <- "World"

new2$data_channel <- as.factor(new2$data_channel)
```

```{r}
new2$day <- rep("Monday", nrow(new2))
new2$day[new2$weekday_is_tuesday == 1] <- "Tuesday"
new2$day[new2$weekday_is_wednesday == 1] <- "Wednesday"
new2$day[new2$weekday_is_thursday == 1] <- "Thursday"
new2$day[new2$weekday_is_friday == 1] <- "Friday"
new2$day[new2$weekday_is_saturday == 1] <- "Saturday"
new2$day[new2$weekday_is_sunday == 1] <- "Sunday"

new2$day <- as.factor(new2$day)
```

```{r}
#looking at the publishing day of the article
new2 <- new2 %>%
  mutate(weekend=as.factor(is_weekend)) %>%
  mutate(day_is_weekend=fct_collapse(weekend,"yes"= "1","no"= "0"))
```

```{r}
classes_df <-
  new2 %>% select(
    -url,
    -timedelta,
    -data_channel_is_lifestyle,
    -data_channel_is_entertainment,
    -data_channel_is_bus,
    -data_channel_is_socmed,
    -data_channel_is_tech,
    -data_channel_is_world,
    -weekday_is_monday,
    -weekday_is_tuesday,
    -weekday_is_wednesday,
    -weekday_is_thursday,
    -weekday_is_friday,
    -weekday_is_saturday,
    -weekday_is_sunday,
    -kw_min_min,
    -kw_max_min,
    -kw_avg_min,
    -kw_min_max,
    -kw_max_max,
    -kw_avg_max,
    -kw_min_avg,
    -kw_max_avg,
    -kw_avg_avg,
    -LDA_00,
    -LDA_01,
    -LDA_02,
    -LDA_03,
    -LDA_04,
    -self_reference_min_shares,
    -self_reference_max_shares,
    -min_positive_polarity,
    -max_positive_polarity,
    -min_negative_polarity,
    -max_negative_polarity,
    -num_self_hrefs,
    -abs_title_subjectivity,
    -abs_title_sentiment_polarity,
    -avg_positive_polarity,
    -avg_negative_polarity,
    -global_rate_negative_words,
    -global_rate_positive_words,
    -weekend,
    -shares
  )
```






