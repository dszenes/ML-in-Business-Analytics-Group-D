# 2 Data

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

## Raw Data Set

<br>

## 2.1 Mashable Data Set

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">
First, we will load the data set that record some sets of features about Mashable articles collected in a period of two years.
</div>

<br>

### Online News Populartiy
```{r}
df <- read_csv(file = here::here("data/OnlineNewsPopularity.csv"))
df <- as_tibble(df)
```

```{r}
df2 <- read.csv(file = here::here("data/OnlineNewsPopularity.csv"))
```
<br>

```{r}
datatable(
  df[1:250, ],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX = T)
)
```

_Source of the data set:_ [https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity]

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Data Set Information:

<br>

* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.

* Acquisition date: January 8, 2015

* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.

<br>

</div>

<br>

* Find below the variables of the data set


  + `url` URL of the article (non-predictive)
  + `timedelta`  Days between the article publication and the dataset acquisition (non-predictive)
  + `n_tokens_title` Number of words in the title 
  + `n_tokens_content` Number of words in the content
  + `n_unique_tokens` Rate of unique words in the content
  + `n_non_stop_words` Rate of non-stop words in the content
  + `n_non_stop_unique_tokens` Rate of unique non-stop words in the content
  + `num_hrefs` Number of links 
  + `num_self_hrefs` Number of links to other articles published by Mashable
  + `num_imgs` Number of images
  + `num_videos` Number of videos
  + `average_token_length` Average length of the words in the content
  + `num_keywords` Number of keywords in the metadata 
  + `data_channel_is_lifestyle` Is data channel 'Lifestyle'?
  + `data_channel_is_entertainment` Is data channel 'Entertainment'?
  + work in progress

<br>

```{r}
#rapid check of the dataset
summary_df <- summary(df)

options(knitr.kable.NA = "") # to hide the non-relevant NAs

# for those interested, you can also use the papeR to produce summaries as nice tables https://cran.r-project.org/web/packages/papeR/vignettes/papeR_introduction.html

summary_df %>% kable_maker(caption = "summary_df")
```

<br>

```{r, fig.height=4, fig.width=8}
par(mfrow = c(3, 4))
for (i in 2:length(df2)) {
  hist(df2[, i], xlab = names(df2)[i])
}
```

<br>

```{r, fig.height=4, fig.width=8}
par(mfrow = c(3, 4))
for (i in 2:length(df2)) {
  boxplot(df2[, i], xlab = names(df2)[i])
}
```

<br>

### Filtering the raw data set

```{r}
zero_df <- df %>%
  filter(n_tokens_content == 0)
```

```{r}
non_zero_df <- df %>%
  filter(n_tokens_content > 0)
```

```{r}
long_df <- df %>%
  filter(n_tokens_content > 700)
```

```{r}
normal_df <- df %>%
  filter(n_tokens_content < 1500 & n_tokens_content > 0)
```

```{r}
normal_df$day <- rep("Monday", nrow(normal_df))
normal_df$day[normal_df$weekday_is_tuesday==1] <- "Tuesday"
normal_df$day[normal_df$weekday_is_wednesday==1] <- "Wednesday"
normal_df$day[normal_df$weekday_is_thursday==1] <- "Thursday"
normal_df$day[normal_df$weekday_is_friday==1] <- "Friday"
normal_df$day[normal_df$weekday_is_saturday==1] <- "Saturday"
normal_df$day[normal_df$weekday_is_sunday==1] <- "Sunday"

ggplot(data=normal_df, aes(as.factor(day), log(shares))) + geom_boxplot()
```

```{r}
normal_df$data_channel <- rep("Lifestyle", nrow(normal_df))
normal_df$data_channel[normal_df$data_channel_is_entertainment==1] <- "Entertainment"
normal_df$data_channel[normal_df$data_channel_is_bus==1] <- "Business"
normal_df$data_channel[normal_df$data_channel_is_socmed==1] <- "Social Media"
normal_df$data_channel[normal_df$data_channel_is_tech==1] <- "Tech"
normal_df$data_channel[normal_df$data_channel_is_world==1] <- "World"
ggplot(data=normal_df, aes(as.factor(data_channel), log(shares))) + geom_boxplot()
```


```{r}
#creating shares popularity classes based on quantiles
quantile(normal_df$shares, probs = c(seq(0,1, length.out = 25)))

share_label <- cut(normal_df$shares, breaks = c(0, 946, 1700, 7200, 843300))
levels(share_label) <- c("Poor", "Average", "Good", "Very Good")

new <- tibble(normal_df, share_label)
```

```{r}
final_df <- new %>% select(-url, -timedelta, -data_channel_is_lifestyle, -data_channel_is_entertainment, -data_channel_is_bus, -data_channel_is_socmed, -data_channel_is_tech, -data_channel_is_world, -weekday_is_monday, -weekday_is_tuesday, -weekday_is_wednesday, -weekday_is_thursday, -weekday_is_friday, -weekday_is_saturday, -weekday_is_sunday, -kw_min_min, -kw_max_min, -kw_avg_min, -kw_min_max, -kw_max_max, -kw_avg_max, -kw_min_avg, -kw_max_avg, -kw_avg_avg, -self_reference_min_shares, -self_reference_max_shares, -min_positive_polarity, -max_positive_polarity, -min_negative_polarity, -max_negative_polarity, -num_self_hrefs, -LDA_00, -LDA_01, -LDA_02, -LDA_03, -LDA_04, -abs_title_subjectivity, -abs_title_sentiment_polarity, -avg_positive_polarity, -avg_negative_polarity)
```




